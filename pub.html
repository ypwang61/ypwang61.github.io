<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Selected Publications</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Yiping Wang</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="pub.html" class="current">Publications</a></div>
<div class="menu-item"><a href="miscellaneous.html">Miscellaneous</a></div>
<div class="menu-item"><a href="fun.html">Fun</a></div>
<div class="menu-item"><a href="CV_YipingWang_phd.pdf">CV</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Selected Publications</h1>
</div>
<p>For the comprehensive list, check out my <a href="https://scholar.google.com/citations?user=IuMFxFUAAAAJ&amp;hl=en&amp;oi=ao">Google Scholar</a> page. <br /></p>
<p><span class="preserve-space">(* denotes equal contribution or alphabetic ordering.)</span> <br /><br /></p>
<table class="imgtable"><tr><td>
<img src="photos/negcliploss.png" alt="alt text" width="400px" height="180px" />&nbsp;</td>
<td align="left"><p><b><a href="https://arxiv.org/abs/2405.19547">CLIPLoss and Norm-Based Data Selection Methods for Multimodal Contrastive Learning</a></b> <span class="preserve-space">   </span><a href="https://github.com/ypwang61/negCLIPLoss_NormSim">[Code]</a> <a href="./pdfs/Poster_negCLIPLoss_NormSim.pdf">[Poster]</a> <a href="https://twitter.com/ypwang61/status/1798396572516151612">[Twitter]</a>  <a href="https://arxiv.org/abs/2402.02055">[Previous Versions]</a> <br />
<b>Yiping Wang</b>*, Yifang Chen*, Wendan Yan, Alex Fang, Wenjing Zhou, Kevin Jamieson, Simon S. Du <br />
<i>NeurIPS 2024  (<font color="red">Spotlight</font>)</i><br /><br />
tl;dr: We design universal data selection methods for CLIP pretraining and achieve near SOTA results with less than 10% of preprocessing resources. It can obtain a new SOTA in <a href="https://www.datacomp.ai/dcclip/leaderboard.html">DataComp benchmark</a> when combined with other approaches.</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="photos/joma.png" alt="alt text" width="400px" height="150px" />&nbsp;</td>
<td align="left"><p><b><a href="https://arxiv.org/abs/2310.00535">JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention</a></b> <span class="preserve-space">   </span><a href="https://twitter.com/tydsh/status/1709785496056930654">[Twitter]</a><br />
Yuandong Tian, <b>Yiping Wang</b>, Zhenyu Zhang, Beidi Chen, Simon S. Du <br />
<i>ICLR 2024</i> <br /><br />
tl;dr: We analyze the training dynamics of multilayer transformer, characterizing the role of self-attention, MLP nonlinearity, and the learning procedure of hierarchical structure, if the data follow hierarchical generative models.<br /><br /></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="photos/scan.png" alt="alt text" width="400px" height="140px" />&nbsp;</td>
<td align="left"><p><b><a href="https://arxiv.org/abs/2305.16380">Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer</a></b> <span class="preserve-space">   </span>  <a href="./pdfs/poster_scan_snap.pdf">[Poster]</a>  <a href="https://twitter.com/tydsh/status/1663611845603885056">[Twitter]</a><br />
Yuandong Tian, <b>Yiping Wang</b>, Beidi Chen, Simon S. Du <br />
<i>NeurIPS 2023</i><br />
<i><font color="red"> Oral </font> presentation at High-dimensional learning dynamics workshop @ ICML 2023</i> <br /><br />
tl;dr: We analyze the 1-layer transformer with next token prediction loss, and rigorously prove its training process and reveal how the token is combined via self-attention layer and the nature of its inductive bias.</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="photos/L1_A_MTRL.png" alt="alt text" width="400px" height="160px" />&nbsp;</td>
<td align="left"><p><b><a href="https://arxiv.org/abs/2306.02556">Improved Active Multi-Task Representation Learning via Lasso</a></b> <br />
<b>Yiping Wang</b>, Yifang Chen, Kevin Jamieson, Simon S. Du <br />
<i>ICML 2023</i> <br /><br />
tl;dr: We improve the sample complexity of active multi-task representation learning by proposing a new LASSO-based strategy.</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="photos/cmixup.png" alt="alt text" width="400px" height="140px" />&nbsp;</td>
<td align="left"><p><b><a href="https://arxiv.org/abs/2210.05775">C-Mixup: Improving Generalization in Regression</a></b> <span class="preserve-space">  </span><a href="https://github.com/huaxiuyao/C-Mixup">[Code]</a> <br />
Huaxiu Yao*, <b>Yiping Wang</b>*, Linjun Zhang, James Zou, Chelsea Finn <br />
<i>NeurIPS 2022</i> <br /><br />
tl;dr: We propose a simple yet effective data augmentation method to improve generalization on regression tasks.</p>
</td></tr></table>
</td>
</tr>
</table>
</body>
</html>
