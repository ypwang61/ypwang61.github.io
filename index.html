<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="icon" type="image/png" sizes="32x32" href="photos/22_icon.png">
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Yiping Wang 王宜平</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Yiping Wang</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="pub.html">Publications</a></div>
<!-- <div class="menu-item"><a href="blog&thoughts.html">Blog & Thoughts</a></div> -->
<div class="menu-item"><a href="CV_YipingWang_phd.pdf">CV</a></div>
<div class="menu-item"><a href="miscellaneous.html">Miscellaneous</a></div>
<div class="menu-item"><a href="fun.html">Fun</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Yiping Wang 王宜平</h1>
</div>

<table class="imgtable"><tr><td>
<!-- <img src="photos/sunshine2.png" alt="alt text" width="190px" height="240px" />&nbsp;</td> -->
<img src="photos/bio_1024_2025.jpg" alt="alt text" width="240px" height="320px" />&nbsp;</td>
<td align="left"><p>Yiping Wang<br />
Ph.D student<br /> <a href="https://www.cs.washington.edu/">Paul G. Allen School of Computer Science &amp; Engineering</a>, <br />
<a href="https://www.washington.edu/">University of Washington</a><br />
Email: ypwang61@cs.washington.edu <br /><br />
<a href="https://scholar.google.com/citations?user=IuMFxFUAAAAJ&amp;hl=en&amp;oi=ao">Google Scholar</a> / <a href="https://twitter.com/ypwang61">X</a> / <a href="https://github.com/ypwang61">Github</a> / <a href="https://www.linkedin.com/in/yiping-wang-323647294/">LinkedIn</a><br /></p>
</td></tr></table>


<h2>About me</h2>
<p>
  I'm a Ph.D. student in <a href="https://www.cs.washington.edu/">Paul G. Allen School of Computer Science &amp; Engineering</a> at <a href="https://www.washington.edu/">University of Washington</a> starting from Fall 2023.
  I feel very fortunate to work with <a href="https://simonshaoleidu.com/index.html">Prof. Simon Shaolei Du</a>.
  Prior to UW, I studied Computer Science and Mathematics in <a href="https://www.zju.edu.cn/english/">Zhejiang University</a>, got an honors degree from <a href="http://ckc.zju.edu.cn/ckcen/_t1906/main.psp">Chu Kochen Honors College</a>.
</p>
<p>
  I'm grateful to all my collaborators and mentors along the way.
  I'm privileged to have been working closely with <a href="http://yuandong-tian.com/">Dr. Yuandong Tian</a> since Spring 2023. 
  I've been interning at Microsoft since June 2024, where I'm fortunate to be advised by <a href="https://scholar.google.com/citations?user=S6OFEFEAAAAJ">Yelong Shen</a> and <a href="https://sites.google.com/site/shuohangsite/">Shuohang Wang</a>.
  During my undergraduate, I was fortunate to work with <a href="https://www.huaxiuyao.io/">Prof. Huaxiu Yao</a> and <a href="https://linjunz.github.io/">Prof. Linjun Zhang</a>.</p>
</p>
<p>
  My long-term research goal is to develop safe AI systems with super-human capabilities that can drive significant scientific progress. 
</p>

<!-- <h2>Research Interest</h2> -->

<!-- <p>
  I'm particularly interested in:
  <ul>
    <li><b>LLM RL</b>: Understanding of RLVR (<a href="https://arxiv.org/abs/2504.20571">One-Shot RLVR</a>, <a href="https://arxiv.org/abs/2506.10947">Spurious Reward</a>)</li>
    <li><b>Multimodal</b>: </li>
  </ul>
    <b>Reinforcement Learning for Large Language Model Reasoning</b> (<a href="https://arxiv.org/abs/2504.20571">One-Shot RLVR</a>, <a href="https://arxiv.org/abs/2506.10947">Spurious Reward</a>)
    and am currently exploring <b>AI4Math</b>.
  My research pursues safe AI systems with super-human reasoning capabilities that can drive independent scientific progress.

</p> -->


<!-- <p>
  I'm particularly interested in 
    <b>Reinforcement Learning for LLM reasoning</b>, like understanding phenomenon in RLVR (<a href="https://arxiv.org/abs/2504.20571">One-Shot RLVR</a>, <a href="https://arxiv.org/abs/2506.10947">Spurious Reward</a>)
    I'm currently exploring <b>AI4Math</b> and self-evolution AI.
    
  I also explore diverse topics before, including
    <b>theoretical understanding of LLMs</b> (<a href="https://arxiv.org/abs/2305.16380">Scan&Snap</a>, <a href="https://arxiv.org/abs/2310.00535">JoMA</a>), 
    <b>data selection algorithms</b> (<a href="https://arxiv.org/abs/2405.19547">CLIPLoss</a>),
    <b>video generation</b> (<a href="https://arxiv.org/abs/2412.16211">StoryEval</a>),
    and <b>general machine learning (theory)</b> 
  In general, I'm excited about developing safe AI systems with super-human reasoning capabilities that can drive independent scientific progress.</p>
<p> -->


<h2>News</h2>
<ul>
<!-- <li><p>
    09/2025: Our <a href="https://arxiv.org/abs/2504.20571">One-Shot-RLVR</a> is accepted by NeurIPS 2025.
</p></li> -->
<li><p>
    10/2025: Get <i>Amazon AI Ph.D. Fellowship</i>, thanks Amazon!
</p></li>
<li><p>
    08/2025: Give a talk on <a href="https://arxiv.org/abs/2504.20571">One-Shot RLVR</a> at a group meeting at Tsinghua University.
</p></li>
<li><p>
   05/2025: Release <a href="https://arxiv.org/abs/2506.10947">Spurious Rewards</a>, which uses RLVR with random reward to incentivize the reasoning capability of pretrained models.
</p></li>
<li><p>
    05/2025: Present <a href="https://arxiv.org/abs/2504.20571">One-Shot RLVR</a> in <a href="https://event.baai.ac.cn/activities/932">BAAI Talk</a>. 
</p></li>
<li><p>
    04/2025: Release <a href="https://arxiv.org/abs/2504.20571">One-Shot RLVR</a> (<a href="https://github.com/ypwang61/One-Shot-RLVR">Code</a>, <a href="https://x.com/ypwang61/status/1917596101953348000">X</a>), rank as #1 Paper of the day on <a href="https://huggingface.co/papers/2504.20571">HuggingFace Daily Papers</a>! We find that with a strong base model, RLVR can improve LLM reasoning with only one proper training example.
</p></li>
<!-- <li><p>
    04/2025: <a href="https://arxiv.org/abs/2505.05950">FloE</a> is accepted by ICML 2025.
</p></li>
<li><p>
    02/2025: <a href="https://arxiv.org/abs/2412.16211">StoryEval</a> is accepted by CVPR 2025.
</p></li> -->
<li><p>
    12/2024: Release a new video generation benchmark <a href="https://ypwang61.github.io/project/StoryEval/">StoryEval</a>, showing that current top video generative models can not present multi-event stories like "How to Put an Elephant in a Refrigerator".
</p></li>
<!-- <li><p>
    12/2024: Attending NeurIPS 2024 in Vancouver and presenting our <a href="https://arxiv.org/abs/2405.19547">CLIPLoss</a> paper!
</p></li> -->
<li><p>
    09/2024: Attend MoDL 2024 in New York sponsored by Simons Foundation, and presenting <a href="https://arxiv.org/abs/2405.19547">CLIPLoss</a> (NeurIPS 2024 <font color="red">spotlight</font>).
</p></li>
<!-- <li><p>
    09/2024: Our <a href="https://arxiv.org/abs/2405.19547">CLIPLoss</a> is accepted by NeurIPS 2024 as <font color="red">spotlight</font>!
</p></li> -->
<li><p>
    06/2024: Start my internship at Microsoft!
</p></li>
<!-- <li><p>
    01/2024: One paper (<a href="https://arxiv.org/abs/2310.00535">JoMA</a>) is accepted by ICLR 2024.
</p></li> -->
<!-- <li><p>
    12/2023: Attended NeurIPS 2023 in New Orleans!
</p></li> -->
<!-- <li><p>
    09/2023: One paper (<a href="https://arxiv.org/abs/2305.16380">Scan&amp;Snap</a>) is accepted by NeurIPS 2023.
</p></li> -->
<li><p>
    05/2024: Release <a href="https://arxiv.org/abs/2405.19547">CLIPLoss</a>, which designs a simple but efficient data selection methods for CLIP pretraining, gets the new SOTA in <a href="https://www.datacomp.ai/dcclip/leaderboard.html">DataComp benchmark</a>.</p>
</p></li>
<li><p>
    10/2023: Release <a href="https://arxiv.org/abs/2310.00535">JoMA</a>, which analyzes the training dynamics of multilayer transformer and characterizes the role of self-attention and MLP nonlinearity.
</p></li> 
<li><p>
    09/2023: Become a husky in UW!
</p></li> 
<li><p>
    05/2023: Release <a href="https://arxiv.org/abs/2305.16380">Scan&amp;Snap</a>, which analyzes the training dynamics of 1-layer linear transformer with next token prediction loss.
</p></li> 
</ul>




<template id="draft-section">
<h2>Main Research</h2>

<p><span class="preserve-space">(* denotes equal contribution or alphabetic ordering, &dagger; denotes corresponding author)</span> <br /><br /></p>
<p><span class="topic-head">LLM RL</span></p>

<p><div class="boxed">
We analyze the empirical observations of Reinforcement Learning with Verifiable Rewards (RLVR) on Large Language Models (LLMs).
</p>

<table class="imgtable"><tr><td>
<img src="photos/one_shot_rlvr.png" alt="alt text" width="300px" height="180px" />&nbsp;</td>
<td align="left"><p>
  <b>Reinforcement Learning for Reasoning in Large Language Models with One Training Example</b>
<br>
<b>Yiping Wang&dagger;</b>, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang&dagger;, Simon Shaolei Du&dagger;, Yelong Shen&dagger;
<br>
<i>Preprint</i>
<br>
<a href="https://arxiv.org/abs/2504.20571" >[Arxiv]</a> 
<a href="https://github.com/ypwang61/One-Shot-RLVR" >[Code]</a> 
<a href="https://wandb.ai/yipingwanguw/verl_few_shot?nw=nwuseryipingwang22" >[W&B]</a> 
<!-- <a href="./pdfs/poster_storyEval_final.pdf", style="color: #666666">[Poster]</a> -->
<a href="https://x.com/ypwang61/status/1917596101953348000" >[X]</a>
<!-- <a href="https://ypwang61.github.io/project/StoryEval/" style="color: #666666">[Website]</a> -->
<br><br>
<!-- tl;dr: We design universal data selection methods for CLIP pretraining and achieve near SOTA results with less than 10% of preprocessing resources. It can obtain a new SOTA in <a href="https://www.datacomp.ai/dcclip/leaderboard.html">DataComp benchmark</a> when combined with other approaches.</p> -->
tl;dr: We show that for RLVR on LLMs, one proper training example can already bring non-trivial improvement.
</td></tr></table>
<p></div></p>

<br>

<p><span class="topic-head">
  Multimodal
</span></p>
<p><div class="boxed">
<!-- We studied how to efficiently select data for multimodal pretraining tasks, drawing inspiration from both empirical observations and theoretical insights. -->
 We study the data selection for multimodal contrastive learning, and investigate the problems of long video generation.
</p>

<table class="imgtable"><tr><td>
<img src="photos/storyeval.gif" alt="alt text" width="300px" height="180px" />&nbsp;</td>
<td align="left"><p>
  <b>Is Your World Simulator a Good Story Presenter? A Consecutive Events-Based Benchmark for Future Long Video Generation</b>
<br>
<b>Yiping Wang</b>, Xuehai He, Kuan Wang, Luyao Ma, Jianwei Yang, Shuohang Wang, Simon Shaolei Du, Yelong Shen
<br>
<i>CVPR 2025</i>
<br>
<a href="https://arxiv.org/abs/2412.16211" >[Arxiv]</a> 
  <a href="https://github.com/ypwang61/StoryEval" >[Code]</a> 
  <a href="./pdfs/poster_storyEval_final.pdf", >[Poster]</a>
  <a href="https://x.com/ypwang61/status/1877079012742144276" >[X]</a>
  <a href="https://ypwang61.github.io/project/StoryEval/" >[Website]</a>
<br><br>
<!-- tl;dr: We design universal data selection methods for CLIP pretraining and achieve near SOTA results with less than 10% of preprocessing resources. It can obtain a new SOTA in <a href="https://www.datacomp.ai/dcclip/leaderboard.html">DataComp benchmark</a> when combined with other approaches.</p> -->
tl;dr: Current top video generative models can not present multi-event stories like "How to Put an Elephant in a Refrigerator".
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="photos/negcliploss.png" alt="alt text" width="300px" height="120px" />&nbsp;</td>
<td align="left"><p>
  <b>CLIPLoss and Norm-Based Data Selection Methods for Multimodal Contrastive Learning</b>
<br>
<b>Yiping Wang</b>*, Yifang Chen*, Wendan Yan, Alex Fang, Wenjing Zhou, Kevin Jamieson, Simon Shaolei Du 
<br>
<i> NeurIPS 2024  (<font color="red">Spotlight</font>)</i>
<br>
<a href="https://arxiv.org/abs/2405.19547" >[Arxiv]</a> 
  <a href="https://github.com/ypwang61/negCLIPLoss_NormSim" >[Code]</a> 
  <a href="./pdfs/Poster_negCLIPLoss_NormSim.pdf" >[Poster]</a> 
  <a href="https://twitter.com/ypwang61/status/1798396572516151612" >[X]</a>
  <a href="https://arxiv.org/abs/2402.02055" >[Previous Versions]</a>
<br><br>
tl;dr: We design simple but efficient data selection methods for CLIP pretraining, and get new SOTA in <a href="https://www.datacomp.ai/dcclip/leaderboard.html">DataComp benchmark</a>.</p>
</td></tr></table>

<!-- <table class="imgtable"><tr><td>
<img src="photos/L1_A_MTRL.png" alt="alt text" width="400px" height="140px" />&nbsp;</td>
<td align="left"><p><b><a href="https://arxiv.org/abs/2306.02556">
  Improved Active Multi-Task Representation Learning via Lasso
</a></b>  <span class="preserve-space">  </span>
<a href="https://arxiv.org/abs/2306.02556">[Arxiv]</a>  <br />
<b>Yiping Wang</b>, Yifang Chen, Kevin Jamieson, Simon S. Du <br />
📍<i>ICML 2023</i> <br /><br />
tl;dr: We improve the sample complexity of active multi-task representation learning by proposing a new LASSO-based strategy.</p>
</td></tr></table> -->

<p></div></p>




<br>
<p><span class="topic-head">
  Theory of Transformer Dynamics
</span></p>
<p><div class="boxed">
We attempted to analyze the training dynamics of transformers in a mathematical way.<br /></p>

<table class="imgtable"><tr><td>
<img src="photos/scan.png" alt="alt text" width="300px" height="120px" />&nbsp;</td>
<td align="left"><p>
  <b>Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer</b>
<br>
Yuandong Tian, <b>Yiping Wang</b>, Beidi Chen, Simon Shaolei Du 
<br>
<i>NeurIPS 2023</i> 
(<font color="red">Oral presentation</font> @ ICML2023-HiDL)
<br>
  <a href="https://arxiv.org/abs/2305.16380" >[Arxiv]</a>
  <a href="./pdfs/poster_scan_snap.pdf" >[Poster]</a>
  <a href="https://twitter.com/tydsh/status/1663611845603885056" >[X]</a>
<br><br>
tl;dr: We analyze the 1-layer transformer with next token prediction loss, and rigorously prove its training process.</p>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="photos/joma.png" alt="alt text" width="300px" height="120px" />&nbsp;</td>
<td align="left"><p>
  <b>JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention</b>
<br>
Yuandong Tian, <b>Yiping Wang</b>, Zhenyu Zhang, Beidi Chen, Simon Shaolei Du <br />
<i>ICLR 2024</i>
<br>
  <a href="https://arxiv.org/abs/2310.00535" >[Arxiv]</a>
  <a href="https://twitter.com/tydsh/status/1709785496056930654" >[X]</a>
<br><br>
tl;dr: We analyze the training dynamics of multilayer transformer, characterizing the role of self-attention and MLP nonlinearity.</p>
</td></tr></table>
<p></div></p>

<!-- <br>
<p><span class="topic-head">Video Generation Evaluation</span></p>

<p><div class="boxed">
We explore the common issues existing in the current top video generative models.
</p>

<table class="imgtable"><tr><td>
<img src="photos/storyeval.gif" alt="alt text" width="300px" height="180px" />&nbsp;</td>
<td align="left"><p>
  <b>Is Your World Simulator a Good Story Presenter? A Consecutive Events-Based Benchmark for Future Long Video Generation</b>
<br>
<b>Yiping Wang</b>, Xuehai He, Kuan Wang, Luyao Ma, Jianwei Yang, Shuohang Wang, Simon Shaolei Du, Yelong Shen
<br>
<i>CVPR 2025</i>
<br>
<a href="https://arxiv.org/abs/2412.16211" >[Arxiv]</a> 
  <a href="https://github.com/ypwang61/StoryEval" >[Code]</a> 
  <a href="./pdfs/poster_storyEval_final.pdf", >[Poster]</a>
  <a href="https://x.com/ypwang61/status/1877079012742144276" >[X]</a>
  <a href="https://ypwang61.github.io/project/StoryEval/" >[Website]</a>
<br><br>
tl;dr: Current top video generative models can not present multi-event stories like "How to Put an Elephant in a Refrigerator".
</td></tr></table>
<p></div></p>


</td>
</tr>
</table> -->

</template>


</body>
</html>



